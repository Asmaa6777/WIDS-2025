{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### the notebook in a nutshell \n",
    "- importing \n",
    "software dependencies we need\n",
    "- load_data (please read its description it supposed to be very easy to deal with it)\n",
    "\n",
    "###  preprocesssing \n",
    "- I stored some important variables, then I used log transformation for long tailed distributions\n",
    "\n",
    "- iterative imputer just like what `MAAB` Advised\n",
    "\n",
    "- transormed the categorical features to be from the string type just to avoid some fitting isssues cause by the \n",
    "OneHotEncoder.\n",
    "\n",
    "- droped the `participant_ID` after storing it for stratification\n",
    "\n",
    "- I scaled quantative data and also FMRI data \n",
    "\n",
    "### modeling\n",
    "- base model is logisitc regression with l2 regulization (fancy name for punishing the model to stop overfitting to noise) \n",
    "- wrapped inside a multioutput classifier that just gives it the ability to predict 2 targets simultaneously\n",
    "### validation \n",
    "- 5 splits stratifiedKFold works fine no need for grouping as participants are unique \n",
    "- regular stratified  = 0.5901173137436236\n",
    "- RepeatedStratifiedKFold =0.5865925294336686 with 7 minutese delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          \n",
    "from sklearn.metrics import f1_score  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from scipy.stats import hmean\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer #it might not work directly if not try the following code line\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold,StratifiedKFold,RepeatedStratifiedKFold\n",
    "from pathlib import Path\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "just drop the path of your new data directory nothing more than that. \n",
    "\n",
    "\"\"\"\n",
    "path = r\"C:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\Data\\raw\"\n",
    "\n",
    "def read_data(base_path:str) -> pd.DataFrame :\n",
    "    path = Path(base_path)\n",
    "    trc=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "    trq=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_QUANTITATIVE_METADATA_new.xlsx')\n",
    "    trf=pd.read_csv(path   /'TRAIN_NEW'  / 'TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "    trs=pd.read_excel(path /'TRAIN_NEW'  / 'TRAINING_SOLUTIONS.xlsx')  \n",
    "    tsc=pd.read_excel(path /'TEST'      / 'TEST_CATEGORICAL.xlsx')\n",
    "    tsq=pd.read_excel(path /'TEST'       / 'TEST_QUANTITATIVE_METADATA.xlsx')    \n",
    "    tsf=pd.read_csv(path   /'TEST'       / 'TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')    \n",
    "    sub=pd.read_excel(path / 'SAMPLE_SUBMISSION.xlsx')    \n",
    "    dic=pd.read_excel(path /'Data Dictionary.xlsx')\n",
    "    return trc, trq, trf, trs, tsc, tsq, tsf, sub, dic\n",
    "\n",
    "trc, trq, trf, trs, tsc, tsq, tsf, sub, dic = read_data(base_path=path)\n",
    "\n",
    "# Data Merging \n",
    "cq = pd.merge(trc, trq, on='participant_id', how='left')\n",
    "feat = pd.merge(cq, trf, on='participant_id', how='left')  \n",
    "qc = pd.merge(tsc, tsq, on='participant_id', how='left')\n",
    "train = pd.merge(feat, trs, on='participant_id', how='left') \n",
    "test = pd.merge(qc, tsf, on='participant_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Highlighting important variables. \n",
    "# note that I did't deal with the quantative data trq as categorical\n",
    "# I will use the OneHotEncoder for the categorical data as we have some data trap that I don't think we can use the label encoder for. \n",
    "\n",
    "for feature in trc.columns:\n",
    "    train[feature] = train[feature].astype(object)\n",
    "train_ids = train['participant_id']\n",
    "test_ids = test['participant_id'] # I will store them for later usage in grouping in validation why?  I don't want the same user to appear in both train and test. \n",
    "num_feats = [feature for feature in train.columns if train[feature].dtype == 'float64']\n",
    "cat_feats = [feature for feature in train.columns if train[feature].dtype == 'object'] # seperate categorical and numerical features help me reteriving them later easily for preprocessing.\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "groups = train_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features with positive skewness and non-negative values: apply log transformation\n",
    "# num_feats[:18] Don't transform fMRI data because it has negative values\n",
    "log_features = [f for f in num_feats[:18] if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0.5]  # Only apply to features with significant positive skew\n",
    "\n",
    "# Apply log transformation for the selected features\n",
    "for feature in log_features:\n",
    "    train[feature] = np.log1p(train[feature])  # Apply log(x+1) to handle skewed features\n",
    "    test[feature] = np.log1p(test[feature])    # Apply log(x+1) to test data as well\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking if target columns in train set (adhd and sex) have missing values\n",
    "train_missing_features_to_impute = train[target_cols].columns[train[target_cols].isnull().any()].tolist()\n",
    "print(train_missing_features_to_impute) # print column name that has missing values if any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train has 25 features/columns with missing values: ['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'MRI_Track_Scan_Location', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan']\n",
    "Test  has 23 features/columns with missing values: ['PreInt_Demos_Fam_Child_Ethnicity', 'PreInt_Demos_Fam_Child_Race', 'Barratt_Barratt_P1_Edu', 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Edu', 'Barratt_Barratt_P2_Occ', 'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP', 'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD', 'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems', 'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems', 'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact', 'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing', 'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial']\n",
    "fMRI has no missing values\n",
    "Extra columns in train: ['MRI_Track_Age_at_Scan', 'MRI_Track_Scan_Location']\n",
    "'''\n",
    "\n",
    "# Find columns with missing values only\n",
    "train_missing_features_to_impute = train.columns[train.isnull().any()].tolist() # List of features with missing values in train, only 25 and no missing data in fMRI data\n",
    "test_missing_features_to_impute = test.columns[test.isnull().any()].tolist() # List of features with missing values in test, only 23 and no missing data in fMRI data\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = IterativeImputer(estimator=LassoCV(random_state=42), max_iter=5, random_state=42)\n",
    "\n",
    "# Impute in-place\n",
    "train[train_missing_features_to_impute] = imputer.fit_transform(train[train_missing_features_to_impute])\n",
    "test[test_missing_features_to_impute] = imputer.fit_transform(test[test_missing_features_to_impute])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical features to strings (to avoid mixed types)\n",
    "for feature in cat_feats:\n",
    "    train[feature] = train[feature].astype(str)\n",
    "    test[feature] = test[feature].astype(str)\n",
    "\n",
    "# One-Hot Encoding for categorical features\n",
    "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "\n",
    "for feature in cat_feats:\n",
    "    if feature == 'participant_id':  # Skip participant_id since it's not a feature\n",
    "        continue\n",
    "\n",
    "    # Apply OneHotEncoder\n",
    "    train_encoded = encoder.fit_transform(train[[feature]])\n",
    "    test_encoded = encoder.transform(test[[feature]])\n",
    "\n",
    "    # Convert encoded features to DataFrame and append them to the original data\n",
    "    train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "    test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "\n",
    "    # Drop the original feature and concatenate the new encoded columns\n",
    "    train = pd.concat([train.drop(columns=[feature]), train_encoded_df], axis=1)\n",
    "    test = pd.concat([test.drop(columns=[feature]), test_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in (train,test):\n",
    "    df.drop(columns=['participant_id'], inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Only apply scaling to numerical columns that are not part of the target or categorical features\n",
    "numerical_features = [col for col in train.columns if col not in target_cols and col not in cat_feats]\n",
    "\n",
    "# Fit scaler on the numerical features of the train set and transform train and test sets\n",
    "train[numerical_features] = scaler.fit_transform(train[numerical_features])  # Fit and transform for train set\n",
    "test[numerical_features] = scaler.transform(test[numerical_features])        # Only transform for test set (avoid data leakage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ayo so there is this modeling thing, the part are \n",
    "- base model is a RidgeClassifier with a regulizer (more on that on comments)\"\n",
    "- multioutput classifier is just a wrapper it sucks the base model and elevate its skills to make it output multiple targets.\n",
    "- there is a pipeline inside the multiooutput classifier that does the following:\n",
    "1. impute missing values with mean\n",
    "2. log transform the features that are skewed so bad (you know why now)\n",
    "3. scale the features to be between 0 and 1\n",
    "4. PCA to reduce the dimensionality of the features (970 components)\n",
    "   \n",
    "\"\"\"\n",
    "features = test.columns \n",
    "n_splits = 5 \n",
    "cv = StratifiedKFold(n_splits=n_splits)\n",
    "base_model = LogisticRegression(random_state=7,penalty='l2', C=0.02,class_weight=\"balanced\")\n",
    "\n",
    "model = MultiOutputClassifier(base_model)\n",
    "\n",
    "# This function will be used to validate the model on one fold at a time, it will called 5 times (5 folds)\n",
    "def validate(trainset, testset, target_cols):\n",
    "    \"\"\"This function takes in:\n",
    "    trainset: your training data (features + targets)\n",
    "    testset: your test data (same structure)\n",
    "    target_cols: list of target column names (because it's a multi-output model)\n",
    "    \"\"\"\n",
    "    weights = ((trainset['Sex_F'] == 1) & (trainset['ADHD_Outcome'] == 1)).astype(int) + 1\n",
    "    model.fit(trainset.drop(columns=target_cols), trainset[target_cols],sample_weight=weights)\n",
    "    pred = model.predict(testset.drop(columns=target_cols))\n",
    "    valid_idx = testset[target_cols].notna().all(axis=1)\n",
    "    \n",
    "    valid_testset = testset.loc[valid_idx, target_cols]\n",
    "    valid_pred = pred[valid_idx]\n",
    "    f1_scores = [f1_score(valid_testset[col], valid_pred[:, i]) for i, col in enumerate(target_cols)]\n",
    "    \n",
    "    # print(f\"F1-scores per target: {dict(zip(target_cols, f1_scores))}\")\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "stds = [] # List to store the standard deviation of each target column in every test split\n",
    "F1s = [] # List to store the F1 scores returned by your validate() function for each fold\n",
    "\n",
    "# Cross-validation loop: Stratified K-Fold\n",
    "\"\"\"\n",
    "# Skf ensures the distribution of the **ADHD_Outcome** target is balanced across the folds. This helps ensure that each fold has a similar proportion of the target classes (e.g., ADHD = 1 and ADHD = 0).\n",
    "# However, **Sex_F**  is not explicitly balanced during this stratified split. The stratification only considers balancing **ADHD_Outcome**.\n",
    "# To make SKF balance distribution of Sex_F instead of adhd, uncomment the following line:\n",
    "\n",
    "for train_index, test_index in cv.split(train.drop(columns=target_cols), train[target_cols[1]]): \n",
    "\"\"\"\n",
    "for fold_number, (train_index, test_index) in enumerate(cv.split(train.drop(columns=target_cols), train[target_cols[0]]), 1):\n",
    "    train_v, test_v = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    # Get the F1 scores for ADHD and Sex_F\n",
    "    f1_scores = validate(train_v, test_v, target_cols)\n",
    "    \n",
    "    # Print fold number and F1 scores\n",
    "    print(f\"\\n==== Fold {fold_number} ===\")\n",
    "    for i, col in enumerate(target_cols):\n",
    "        print(f\"{col} -> F1: {f1_scores[i]:.4f}\")\n",
    "    \n",
    "    # Append standard deviations of the test set to the stds list\n",
    "    stds.append(test_v[target_cols].std())\n",
    "    \n",
    "    # Append F1 scores to the F1s list\n",
    "    F1s.append(f1_scores)\n",
    "\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "mean_f1_scores = F1s.mean(axis=0)\n",
    "print(\"\\n==== Overall Results ===\")\n",
    "print(\"Mean F1-scores:\")\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(f\"    {target} -> F1: {mean_f1_scores[i]:.4f}\")\n",
    "    \n",
    "    \n",
    "f1_stds = F1s.std(axis=0)\n",
    "\n",
    "print(\"\\nF1-score stds:\")\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(f\"    {target} -> Std: {f1_stds[i]:.4f}\")\n",
    "    \n",
    "print(\"\\nStandard deviations of test sets:\")\n",
    "for std in stds:\n",
    "    print(f\"    {std.to_dict()}\")\n",
    "\n",
    "print(\"\\nscore mean (ADHD Sex): \", np.mean(F1s, axis=0))\n",
    "score = hmean(F1s, axis=0)\n",
    "score = hmean(score, axis=0)\n",
    "print(\"\\nFinal Overall Harmonic Mean (Mimic Leaderboard): \", score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying my own code on Ahmed's validation to see if results are similar. Using LogisticRegressionCV Stratify on ADHD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ayo so there is this modeling thing, the parts are:\n",
    "- now we train two separate models instead of using MultiOutputClassifier\n",
    "- each model is a LogisticRegressionCV that tunes regularization using cross-validation\n",
    "- preprocessing still applies before this (pipeline, imputation, scaling, PCA, etc.)\n",
    "\"\"\"\n",
    "\n",
    "# Constants\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "n_splits = 5\n",
    "SEED = 42  # set your seed if not defined elsewhere\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Validation function for two independent models\n",
    "def validate(trainset, testset, target_cols):\n",
    "    f1_scores = []\n",
    "\n",
    "    for target in target_cols:\n",
    "        # LogisticRegressionCV for this target\n",
    "        model = LogisticRegressionCV(\n",
    "            penalty=\"l2\",\n",
    "            Cs=10,\n",
    "            cv=5,\n",
    "            fit_intercept=True,\n",
    "            scoring=\"f1\",\n",
    "            solver=\"saga\",\n",
    "            random_state=SEED,\n",
    "            class_weight=\"balanced\",\n",
    "            max_iter=1000\n",
    "        )\n",
    "\n",
    "        # Define weights if needed (only for ADHD, in this case)\n",
    "        if target == 'ADHD_Outcome':\n",
    "            weights = ((trainset['Sex_F'] == 1) & (trainset['ADHD_Outcome'] == 1)).astype(int) + 1\n",
    "            model.fit(trainset.drop(columns=target_cols), trainset[target], sample_weight=weights)\n",
    "        else:\n",
    "            model.fit(trainset.drop(columns=target_cols), trainset[target])\n",
    "\n",
    "        # Predict\n",
    "        preds = model.predict(testset.drop(columns=target_cols))\n",
    "\n",
    "        # Mask valid rows (non-NaN)\n",
    "        valid_idx = testset[target].notna()\n",
    "        true_vals = testset.loc[valid_idx, target]\n",
    "        valid_preds = preds[valid_idx]\n",
    "\n",
    "        # Compute F1\n",
    "        score = f1_score(true_vals, valid_preds)\n",
    "        f1_scores.append(score)\n",
    "\n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "for fold_number, (train_index, test_index) in enumerate(cv.split(train.drop(columns=target_cols), train[target_cols[0]]), 1):\n",
    "    train_v, test_v = train.iloc[train_index], train.iloc[test_index]\n",
    "\n",
    "    f1_scores = validate(train_v, test_v, target_cols)\n",
    "\n",
    "    print(f\"\\n==== Fold {fold_number} ===\")\n",
    "    for i, col in enumerate(target_cols):\n",
    "        print(f\"{col} -> F1: {f1_scores[i]:.4f}\")\n",
    "\n",
    "    stds.append(test_v[target_cols].std())\n",
    "    F1s.append(f1_scores)\n",
    "\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "mean_f1_scores = F1s.mean(axis=0)\n",
    "print(\"\\n==== Overall Results ===\")\n",
    "print(\"Mean F1-scores:\")\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(f\"    {target} -> F1: {mean_f1_scores[i]:.4f}\")\n",
    "\n",
    "f1_stds = F1s.std(axis=0)\n",
    "print(\"\\nF1-score stds:\")\n",
    "for i, target in enumerate(target_cols):\n",
    "    print(f\"    {target} -> Std: {f1_stds[i]:.4f}\")\n",
    "\n",
    "print(\"\\nStandard deviations of test sets:\")\n",
    "for std in stds:\n",
    "    print(f\"    {std.to_dict()}\")\n",
    "\n",
    "print(\"\\nscore mean (ADHD, Sex):\", np.mean(F1s, axis=0))\n",
    "score = hmean(F1s, axis=0)\n",
    "score = hmean(score, axis=0)\n",
    "print(\"\\nFinal Overall Harmonic Mean (Mimic Leaderboard):\", score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(testdata, model_adhd, model_sex):\n",
    "    # Fit ADHD model\n",
    "    model_adhd.fit(train.drop(columns=[\"ADHD_Outcome\", \"Sex_F\"]), train[\"ADHD_Outcome\"])\n",
    "    \n",
    "    # Fit Sex model\n",
    "    model_sex.fit(train.drop(columns=[\"ADHD_Outcome\", \"Sex_F\"]), train[\"Sex_F\"])\n",
    "    \n",
    "    # Predict\n",
    "    adhd_pred = model_adhd.predict(test)\n",
    "    sex_pred = model_sex.predict(test)\n",
    "\n",
    "    # Prepare submission\n",
    "    sub['ADHD_Outcome'] = adhd_pred\n",
    "    sub['Sex_F'] = sex_pred\n",
    "    sub.to_csv(f'../submission{score}.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "model_adhd = LogisticRegressionCV(\n",
    "    penalty=\"l2\",\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    fit_intercept=True,\n",
    "    scoring=\"f1\",\n",
    "    solver=\"saga\",\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "model_sex = LogisticRegressionCV(\n",
    "    penalty=\"l2\",\n",
    "    Cs=10,\n",
    "    cv=5,\n",
    "    fit_intercept=True,\n",
    "    scoring=\"f1\",\n",
    "    solver=\"saga\",\n",
    "    random_state=SEED,\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=1000\n",
    ")\n",
    "\n",
    "inference(test, model_adhd, model_sex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(testdata, model):\n",
    "\n",
    "    model.fit(train.drop(columns=[\"ADHD_Outcome\", \"Sex_F\"], axis=1), train[[\"ADHD_Outcome\", \"Sex_F\"]])\n",
    "    y_pred = model.predict(test)\n",
    "    sub['ADHD_Outcome'] = y_pred[:, 0] \n",
    "    sub['Sex_F'] = y_pred[:, 1]        \n",
    "    sub.to_csv(f'../submission{score}.csv', index=False)\n",
    "    \n",
    "inference(test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
