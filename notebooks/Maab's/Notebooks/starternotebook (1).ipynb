{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np          \n",
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from sklearn.metrics import f1_score  \n",
    "from sklearn.pipeline import make_pipeline  \n",
    "from sklearn.impute import SimpleImputer  \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from lightgbm import LGBMClassifier\n",
    "from scipy.stats import hmean\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from pathlib import Path\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.experimental import enable_iterative_imputer  # This line enables it\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "just drop the path of your new data directory nothing more than that. \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "path = r\"C:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\Data\\raw\"\n",
    "\n",
    "def read_data(base_path:str) -> pd.DataFrame :\n",
    "    path = Path(base_path)\n",
    "    trc=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_CATEGORICAL_METADATA_new.xlsx')\n",
    "    trq=pd.read_excel(path /'TRAIN_NEW'  / 'TRAIN_QUANTITATIVE_METADATA_new.xlsx')\n",
    "    trf=pd.read_csv(path   /'TRAIN_NEW'  / 'TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv')\n",
    "    trs=pd.read_excel(path /'TRAIN_NEW'  / 'TRAINING_SOLUTIONS.xlsx')  \n",
    "    tsc=pd.read_excel(path /'TEST'      / 'TEST_CATEGORICAL.xlsx')\n",
    "    tsq=pd.read_excel(path /'TEST'       / 'TEST_QUANTITATIVE_METADATA.xlsx')    \n",
    "    tsf=pd.read_csv(path   /'TEST'       / 'TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv')    \n",
    "    sub=pd.read_excel(path / 'SAMPLE_SUBMISSION.xlsx')    \n",
    "    dic=pd.read_excel(path /'Data Dictionary.xlsx')\n",
    "    return trc, trq, trf, trs, tsc, tsq, tsf, sub, dic\n",
    "\n",
    "trc, trq, trf, trs, tsc, tsq, tsf, sub, dic = read_data(base_path=path)\n",
    "\n",
    "# Data Merging \n",
    "cq = pd.merge(trc, trq, on='participant_id', how='left')\n",
    "feat = pd.merge(cq, trf, on='participant_id', how='left')  \n",
    "qc = pd.merge(tsc, tsq, on='participant_id', how='left')\n",
    "train = pd.merge(feat, trs, on='participant_id', how='left') \n",
    "test = pd.merge(qc, tsf, on='participant_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'participant_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\venv2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'participant_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# highlighting important variables. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#note that I did't deal with the quantative data trq as categorical\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# I will use the OneHotEncoder for the categorical data as we have some data trap that I don't think we can use the label encoder for. \u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m trc\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m----> 6\u001b[0m     train[feature] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\n\u001b[0;32m      7\u001b[0m train_ids \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      8\u001b[0m test_ids \u001b[38;5;241m=\u001b[39m train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparticipant_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;66;03m# I will store them for later usage in grouping in validation why?  I don't want the same user to appear in both train and test. \u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\venv2\\lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\venv2\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'participant_id'"
     ]
    }
   ],
   "source": [
    "# highlighting important variables. \n",
    "#note that I did't deal with the quantative data trq as categorical\n",
    "# I will use the OneHotEncoder for the categorical data as we have some data trap that I don't think we can use the label encoder for. \n",
    "\n",
    "for feature in trc.columns:\n",
    "    train[feature] = train[feature].astype(object)\n",
    "train_ids = train['participant_id']\n",
    "test_ids = train['participant_id'] # I will store them for later usage in grouping in validation why?  I don't want the same user to appear in both train and test. \n",
    "num_feats = [feature for feature in train.columns if train[feature].dtype == 'float64']\n",
    "cat_feats = [feature for feature in train.columns if train[feature].dtype == 'object'] # seperate categorical and numerical features help me reteriving them later easily for preprocessing.\n",
    "target_cols = ['ADHD_Outcome', 'Sex_F']\n",
    "groups = train_ids\n",
    "log_features = [f for f in num_feats if (train[f] >= 0).all() and scipy.stats.skew(train[f]) > 0] # I will apply log transformation for the skewed features (basic preprocessing our models assume normal bell curve probability distribution)\n",
    "\n",
    "encoder = OneHotEncoder(handle_unknown='ignore',sparse_output=False)\n",
    "le = LabelEncoder()\n",
    "for feature in log_features:\n",
    "    train[feature] = np.log1p(train[feature])\n",
    "    test[feature] = np.log1p(test[feature])\n",
    "    \n",
    "from sklearn.experimental import enable_iterative_imputer  # This line enables it\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Define which features you want to impute (everything except target columns)\n",
    "features_to_impute = [f for f in train.columns if f not in target_cols]\n",
    "\n",
    "# Create the imputer\n",
    "imputer = IterativeImputer(random_state=0)\n",
    "\n",
    "# Fit the imputer on train set and transform both train and test\n",
    "train[features_to_impute] = imputer.fit_transform(train[features_to_impute])\n",
    "test[features_to_impute] = imputer.transform(test[features_to_impute])\n",
    "\n",
    "# Convert all categorical features to strings to avoid mixed types\n",
    "for feature in cat_feats:\n",
    "    train[feature] = train[feature].astype(str)\n",
    "    test[feature] = test[feature].astype(str)\n",
    "\n",
    "for feature in cat_feats:\n",
    "    if feature != 'participant_id':\n",
    "        \n",
    "        train_encoded = encoder.fit_transform(train[[feature]])\n",
    "        test_encoded = encoder.transform(test[[feature]])\n",
    "        print(f\"Encoding {feature} with OneHotEncoder\")\n",
    "        train_encoded_df = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "        test_encoded_df = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out([feature]))\n",
    "        train = pd.concat([train.drop(columns=[feature]), train_encoded_df], axis=1)\n",
    "        test = pd.concat([test.drop(columns=[feature]), test_encoded_df], axis=1)\n",
    "\n",
    "        \n",
    "    else:\n",
    "        train[feature] = le.fit_transform(train[feature])\n",
    "        test[feature] = le.fit_transform(test[feature])        \n",
    "  \n",
    "for df in (train,test):\n",
    "    df.drop(columns=['participant_id'], inplace=True)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling features\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "for col in train.columns:\n",
    "    if col not in target_cols and col not in cat_feats:\n",
    "        train[col] = scaler.fit_transform(train[[col]])\n",
    "        test[col] = scaler.transform(test[[col]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ADHD_Outcome</th>\n",
       "      <th>Sex_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1210</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1212</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1213 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      ADHD_Outcome  Sex_F\n",
       "0                1      0\n",
       "1                1      0\n",
       "2                0      1\n",
       "3                0      1\n",
       "4                1      0\n",
       "...            ...    ...\n",
       "1208             0      1\n",
       "1209             1      0\n",
       "1210             1      1\n",
       "1211             1      0\n",
       "1212             0      1\n",
       "\n",
       "[1213 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[target_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-scores per target: {'ADHD_Outcome': 0.7051671732522796, 'Sex_F': 0.5272727272727272}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7283950617283951, 'Sex_F': 0.5}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6729559748427673, 'Sex_F': 0.50920245398773}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.6729559748427673, 'Sex_F': 0.4875}\n",
      "F1-scores per target: {'ADHD_Outcome': 0.7244582043343654, 'Sex_F': 0.5288753799392097}\n",
      "Mean F1-scores per target: [0.70078648 0.51057011]\n",
      "F1-score stds per target: [0.02404516 0.0158738 ]\n",
      "Standard deviations of test sets: [ADHD_Outcome    0.466218\n",
      "Sex_F           0.480409\n",
      "dtype: float64, ADHD_Outcome    0.464573\n",
      "Sex_F           0.472377\n",
      "dtype: float64, ADHD_Outcome    0.466218\n",
      "Sex_F           0.475213\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.468331\n",
      "dtype: float64, ADHD_Outcome    0.465098\n",
      "Sex_F           0.480850\n",
      "dtype: float64]\n",
      "score mean  [0.70078648 0.51057011]\n",
      "overall harmonic mean  0.5901173137436236\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "ayo so there is this modeling thing, the part are \n",
    "- base model is a RidgeClassifier with a regulizer (more on that on comments)\"\n",
    "- multioutput classifier is just a wrapper it sucks the base model and elevate its skills to make it output multiple targets.\n",
    "- there is a pipeline inside the multiooutput classifier that does the following:\n",
    "1. impute missing values with mean\n",
    "2. log transform the features that are skewed so bad (you know why now)\n",
    "3. scale the features to be between 0 and 1\n",
    "4. PCA to reduce the dimensionality of the features (970 components)\n",
    "   \n",
    "\"\"\"\n",
    "features = test.columns\n",
    "n_splits = 5 \n",
    "cv = StratifiedGroupKFold(n_splits=n_splits)\n",
    "##base_model = RidgeClassifier(random_state=7, class_weight=\"balanced\")\n",
    "base_model = LogisticRegression(random_state=7,penalty='l2', C=0.02,class_weight=\"balanced\")\n",
    "\n",
    "model = MultiOutputClassifier(base_model)\n",
    "\n",
    "def validate(trainset, testset, target_cols):\n",
    "    weights = (trainset['Sex_F'] == 1) & (trainset['ADHD_Outcome'] == 1) * 2 + 1\n",
    "    model.fit(trainset.drop(columns=target_cols), trainset[target_cols],sample_weight=weights)\n",
    "    pred = model.predict(testset.drop(columns=target_cols))\n",
    "    valid_idx = testset[target_cols].notna().all(axis=1)\n",
    "    \n",
    "    valid_testset = testset.loc[valid_idx, target_cols]\n",
    "    valid_pred = pred[valid_idx]\n",
    "    f1_scores = [f1_score(valid_testset[col], valid_pred[:, i]) for i, col in enumerate(target_cols)]\n",
    "    \n",
    "    print(f\"F1-scores per target: {dict(zip(target_cols, f1_scores))}\")\n",
    "    \n",
    "    return f1_scores\n",
    "\n",
    "\n",
    "stds = []\n",
    "F1s = []\n",
    "\n",
    "for train_index, test_index in cv.split(train.drop(columns=target_cols), train[target_cols[0]], groups=groups): \n",
    "    train_v, test_v = train.iloc[train_index], train.iloc[test_index]\n",
    "    \n",
    "    stds.append(test_v[target_cols].std())\n",
    "    F1s.append(validate(train_v, test_v, target_cols))\n",
    "\n",
    "F1s = np.array(F1s)\n",
    "\n",
    "print(\"Mean F1-scores per target:\", F1s.mean(axis=0))\n",
    "print(\"F1-score stds per target:\", F1s.std(axis=0))\n",
    "print(\"Standard deviations of test sets:\", stds)\n",
    "print(\"score mean \", np.mean(F1s, axis=0))\n",
    "score = hmean(F1s, axis=0)\n",
    "score = hmean(score, axis=0)\n",
    "print(\"overall harmonic mean \", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(testdata, model):\n",
    "\n",
    "    model.fit(train.drop(columns=[\"ADHD_Outcome\", \"Sex_F\"], axis=1), train[[\"ADHD_Outcome\", \"Sex_F\"]])\n",
    "    y_pred = model.predict(test)\n",
    "    sub['ADHD_Outcome'] = y_pred[:, 0] \n",
    "    sub['Sex_F'] = y_pred[:, 1]        \n",
    "    sub.to_csv(f'../submissions/submission{score}.csv', index=False)\n",
    "    \n",
    "inference(test, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
