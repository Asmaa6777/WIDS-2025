{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the data from the input files\n",
    "1. Training Dataset : Merge the quantitative, categorical metadata files & solution data\n",
    "2. Training Dataset : Merge the quantitative, categorical metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set base directory for data\n",
    "BASE_DIR = r\"C:\\Users\\Maab\\Desktop\\ADHD_Kaggle_Competition\\Repo\\WiDS-Datathon-2025\\Data\\raw\"\n",
    "\n",
    "# Function to load all data\n",
    "def get_feats(mode='TRAIN'):\n",
    "    \"\"\"\n",
    "    Loads and merges the relevant datasets based on the mode ('TRAIN' or 'TEST').\n",
    "    \"\"\"\n",
    "\n",
    "    # Define folder path based on mode\n",
    "    folder = os.path.join(BASE_DIR, \"TRAIN_NEW\" if mode == 'TRAIN' else \"TEST\")\n",
    "\n",
    "    # Load quantitative metadata\n",
    "    feats = pd.read_excel(os.path.join(folder, f\"{mode}_QUANTITATIVE_METADATA_new.xlsx\" if mode == 'TRAIN' else f\"{mode}_QUANTITATIVE_METADATA.xlsx\"))\n",
    "\n",
    "    # Load categorical metadata\n",
    "    if mode == 'TRAIN':\n",
    "        cate = pd.read_excel(os.path.join(folder, \"TRAIN_CATEGORICAL_METADATA_new.xlsx\"))\n",
    "    else:\n",
    "        cate = pd.read_excel(os.path.join(folder, \"TEST_CATEGORICAL.xlsx\"))\n",
    "\n",
    "    # Merge categorical data\n",
    "    feats = feats.merge(cate, on='participant_id', how='left')\n",
    "\n",
    "    # Load functional connectome matrices\n",
    "    func_filename = \"TRAIN_FUNCTIONAL_CONNECTOME_MATRICES_new_36P_Pearson.csv\" if mode == 'TRAIN' else \"TEST_FUNCTIONAL_CONNECTOME_MATRICES.csv\"\n",
    "    func = pd.read_csv(os.path.join(folder, func_filename))\n",
    "\n",
    "    # Merge functional data\n",
    "    feats = feats.merge(func, on='participant_id', how='left')\n",
    "\n",
    "    # If training data, merge with solution file\n",
    "    if mode == 'TRAIN':\n",
    "        solution_path = os.path.join(folder, \"TRAINING_SOLUTIONS.xlsx\")\n",
    "        solution = pd.read_excel(solution_path)\n",
    "        feats = feats.merge(solution, on='participant_id', how='left')\n",
    "\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data\n",
    "train = get_feats(mode='TRAIN')\n",
    "test = get_feats(mode='TEST')\n",
    "\n",
    "sub = pd.read_excel(os.path.join(BASE_DIR, 'SAMPLE_SUBMISSION.xlsx'))\n",
    "y = pd.read_excel(os.path.join(BASE_DIR, \"TRAIN_NEW\\TRAINING_SOLUTIONS.xlsx\"))\n",
    "\n",
    "\n",
    "# Set index\n",
    "train.set_index('participant_id', inplace=True)\n",
    "test.set_index('participant_id', inplace=True)\n",
    "\n",
    "# Define targets and features\n",
    "targets = ['ADHD_Outcome', 'Sex_F']\n",
    "\n",
    "non_connectome_features = [\n",
    "    feature for feature in train.columns \n",
    "    if 'throw' not in feature and feature not in targets\n",
    "]\n",
    "\n",
    "connectome_features = [feature for feature in train.columns if 'throw' in feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing columns from train that is not in test\n",
    "y_ADHD = train['ADHD_Outcome']\n",
    "y_Sex = train['Sex_F']\n",
    "\n",
    "train.drop(columns=['ADHD_Outcome', 'Sex_F'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\n",
    "    \"Basic_Demos_Study_Site\", \"MRI_Track_Scan_Location\", 'Barratt_Barratt_P1_Occ', 'Barratt_Barratt_P2_Occ'\n",
    "]\n",
    "\n",
    "train.drop(drop_cols, axis=1, inplace=True)\n",
    "test.drop(drop_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MISSING VALUES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute train and test set for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features with missing values in training data before imputing:\n",
      "                                  Missing Values  Percentage\n",
      "MRI_Track_Age_at_Scan                        360   29.678483\n",
      "Barratt_Barratt_P2_Edu                       198   16.323166\n",
      "PreInt_Demos_Fam_Child_Race                   54    4.451772\n",
      "PreInt_Demos_Fam_Child_Ethnicity              43    3.544930\n",
      "ColorVision_CV_Score                          23    1.896125\n",
      "Barratt_Barratt_P1_Edu                        15    1.236603\n",
      "EHQ_EHQ_Total                                 13    1.071723\n",
      "APQ_P_APQ_P_PP                                12    0.989283\n",
      "APQ_P_APQ_P_PM                                12    0.989283\n",
      "APQ_P_APQ_P_OPD                               12    0.989283\n",
      "APQ_P_APQ_P_INV                               12    0.989283\n",
      "APQ_P_APQ_P_ID                                12    0.989283\n",
      "APQ_P_APQ_P_CP                                12    0.989283\n",
      "SDQ_SDQ_Conduct_Problems                       9    0.741962\n",
      "SDQ_SDQ_Difficulties_Total                     9    0.741962\n",
      "SDQ_SDQ_Emotional_Problems                     9    0.741962\n",
      "SDQ_SDQ_Generating_Impact                      9    0.741962\n",
      "SDQ_SDQ_Hyperactivity                          9    0.741962\n",
      "SDQ_SDQ_Internalizing                          9    0.741962\n",
      "SDQ_SDQ_Peer_Problems                          9    0.741962\n",
      "SDQ_SDQ_Prosocial                              9    0.741962\n",
      "SDQ_SDQ_Externalizing                          9    0.741962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer  # Enables IterativeImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import LassoCV\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress the convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "\n",
    "# Check for missing values\n",
    "missing = train.isnull().sum()\n",
    "missing_percent = 100 * missing / len(train)\n",
    "missing_df = pd.DataFrame({'Missing Values': missing, 'Percentage': missing_percent})\n",
    "\n",
    "# Select columns to impute (first 23 columns, for example)\n",
    "cols_to_impute = train.columns[:23]\n",
    "\n",
    "# Initialize the imputer\n",
    "imputer = IterativeImputer(estimator=LassoCV(random_state=42), max_iter=5, random_state=42)\n",
    "\n",
    "# Impute in-place\n",
    "train[cols_to_impute] = imputer.fit_transform(train[cols_to_impute])\n",
    "test[cols_to_impute] = imputer.transform(test[cols_to_impute])\n",
    "\n",
    "# Print features that originally had missing values\n",
    "missing_features = missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False)\n",
    "print(\"\\nFeatures with missing values in training data before imputing:\")\n",
    "print(missing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features with missing values in testing data:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>Percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Missing Values, Percentage]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the testing data\n",
    "missing = train.isnull().sum()\n",
    "missing_percent = 100 * missing / len(train)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "\n",
    "\n",
    "# Display features with missing values\n",
    "missing_features = missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage', ascending=False)\n",
    "print(\"\\nFeatures with missing values in testing data:\")\n",
    "missing_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "train = pd.DataFrame(\n",
    "    scaler.fit_transform(train), columns=train.columns, index=train.index\n",
    ")\n",
    "test = pd.DataFrame(\n",
    "    scaler.transform(test), columns=test.columns, index=test.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_sex = [\n",
    "    'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP',\n",
    "    'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD',\n",
    "    'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "    'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems',\n",
    "    'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "    'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan'\n",
    "]\n",
    "\n",
    "\n",
    "features_adhd = [\n",
    "    'EHQ_EHQ_Total', 'ColorVision_CV_Score', 'APQ_P_APQ_P_CP',\n",
    "    'APQ_P_APQ_P_ID', 'APQ_P_APQ_P_INV', 'APQ_P_APQ_P_OPD',\n",
    "    'APQ_P_APQ_P_PM', 'APQ_P_APQ_P_PP', 'SDQ_SDQ_Conduct_Problems',\n",
    "    'SDQ_SDQ_Difficulties_Total', 'SDQ_SDQ_Emotional_Problems',\n",
    "    'SDQ_SDQ_Externalizing', 'SDQ_SDQ_Generating_Impact',\n",
    "    'SDQ_SDQ_Hyperactivity', 'SDQ_SDQ_Internalizing',\n",
    "    'SDQ_SDQ_Peer_Problems', 'SDQ_SDQ_Prosocial', 'MRI_Track_Age_at_Scan', \n",
    "    'PreInt_Demos_Fam_Child_Race'\n",
    "]\n",
    "\n",
    "\n",
    "# Features to be interacted with predicted probability of Sex_F = 1\n",
    "interactions = [\n",
    "    \"APQ_P_APQ_P_INV\", \"APQ_P_APQ_P_PP\", \"SDQ_SDQ_Hyperactivity\", \n",
    "    \"MRI_Track_Age_at_Scan\", \"SDQ_SDQ_Generating_Impact\"\n",
    "]\n",
    "\n",
    "combinations = y_ADHD.astype(str) + y_Sex.astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. L1 gives better F1 score for ADHD, but ROC AUC is veeerrrryyyyyy low\n",
    "2. L2 gives better score for sex (both F1 and ROC AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalty L2 Both Basic Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ðŸ§ª Repeated Stratified Cross-Validation with Logistic RegressionCV: Predicting Sex and ADHD\n",
    "This section performs **repeated stratified K-Fold cross-validation** for two related classification tasks using `LogisticRegressionCV`: predicting **sex (Sex_F)** and **ADHD outcome**. It incorporates interaction terms and sample weighting to improve prediction robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”§ Models:\n",
    "- **Model**: `LogisticRegressionCV` with L2 penalty, saga solver, inner StratifiedKFold CV.\n",
    "- **Task 1 (Sex Prediction)**:\n",
    "  - **Target**: `y_Sex`\n",
    "  - **Features**: Full feature set from `train`\n",
    "- **Task 2 (ADHD Prediction)**:\n",
    "  - **Target**: `y_ADHD`\n",
    "  - **Features**: `features_adhd` + `sex_proba` + interactions between `sex_proba` and selected features\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Key Features:\n",
    "- Uses **predicted sex probabilities** as a feature for ADHD prediction.\n",
    "- **Interaction terms**: Multiplicative features between `sex_proba` and selected predictors.\n",
    "- Sample weights:\n",
    "  - Doubles the weight for samples where both labels (`Sex_F` and `ADHD`) are positive (`\"11\"`) to handle data imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”„ Validation Strategy:\n",
    "- **RepeatedStratifiedKFold** (5 folds Ã— 5 repeats)\n",
    "- Performance metrics:\n",
    "  - **F1 Score** (thresholded)\n",
    "  - **ROC-AUC**\n",
    "- Out-of-fold predictions are collected for final performance reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§® Outputs:\n",
    "- Final mean F1 and ROC-AUC for both `Sex_F` and `ADHD` predictions.\n",
    "\n",
    "### ðŸ§  Insights from Cross-Validation\n",
    "\n",
    "### 1. **Sex Prediction (Sex_F) Performance**\n",
    "   - **Mean F1**: 0.7822\n",
    "   - **Mean ROC-AUC**: 0.7872\n",
    "   - **Key Observations**:\n",
    "     - The **F1 scores** for sex prediction vary, with the highest reaching 0.7268 in Fold 22 and the lowest at 0.6534 in Fold 15. This suggests fluctuation in model performance across data splits.\n",
    "     - The **ROC-AUC** remains stable, indicating the model consistently distinguishes between classes with good confidence.\n",
    "     - **Fluctuations in performance** (especially in folds like 15 and 17) suggest the model might struggle with certain data subsets, indicating room for improvement.\n",
    "\n",
    "### 2. **ADHD Outcome Prediction (Outcome ADHD) Performance**\n",
    "   - **Mean F1**: 0.8370\n",
    "   - **Mean ROC-AUC**: 0.8427\n",
    "   - **Key Observations**:\n",
    "     - **F1 scores** for ADHD prediction are consistently strong, with scores above 0.85 in most folds. The highest F1 score (Fold 24) reaches 0.8968, showcasing strong model performance.\n",
    "     - **ROC-AUC** is consistently high, showing the model's strong ability to differentiate between ADHD and non-ADHD cases.\n",
    "\n",
    "### 3. **Comparison of Tasks (Sex vs. ADHD)**\n",
    "   - **ADHD prediction** consistently outperforms **Sex_F** in both **F1** and **ROC-AUC** scores. This could be due to:\n",
    "     - Better separation or more predictable patterns in ADHD data.\n",
    "     - Potentially more informative features for ADHD prediction.\n",
    "\n",
    "### 4. **Model Stability and Performance Fluctuations**\n",
    "   - The **model's performance** fluctuates across folds for both tasks, but **ADHD** consistently outperforms **Sex_F**. \n",
    "   - The **fluctuations** in **Sex_F performance**, particularly in folds 15 and 17, suggest that the model may be struggling with certain data subsets, especially in sex prediction where there is more variance.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ Final Takeaways:\n",
    "- Both models show **promising results**, with **ADHD prediction** being the stronger task overall.\n",
    "- The **Sex_F performance** shows **room for improvement**, especially with the fluctuations in some folds. Consider feature engineering, tuning, or exploring other models to enhance performance.\n",
    "- The **ADHD model** is robust and reliable across folds, demonstrating **solid predictive power** for this task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1 ===\n",
      "Sex_F -> F1: 0.6967, ROC-AUC: 0.7680\n",
      "Outcome ADHD -> F1: 0.8839, ROC-AUC: 0.8603\n",
      "\n",
      "=== Fold 2 ===\n",
      "Sex_F -> F1: 0.7086, ROC-AUC: 0.8125\n",
      "Outcome ADHD -> F1: 0.8812, ROC-AUC: 0.8291\n",
      "\n",
      "=== Fold 3 ===\n",
      "Sex_F -> F1: 0.7082, ROC-AUC: 0.7977\n",
      "Outcome ADHD -> F1: 0.8860, ROC-AUC: 0.8309\n",
      "\n",
      "=== Fold 4 ===\n",
      "Sex_F -> F1: 0.7163, ROC-AUC: 0.7910\n",
      "Outcome ADHD -> F1: 0.8742, ROC-AUC: 0.8572\n",
      "\n",
      "=== Fold 5 ===\n",
      "Sex_F -> F1: 0.6817, ROC-AUC: 0.8046\n",
      "Outcome ADHD -> F1: 0.8945, ROC-AUC: 0.7982\n",
      "\n",
      "=== Fold 6 ===\n",
      "Sex_F -> F1: 0.6908, ROC-AUC: 0.7813\n",
      "Outcome ADHD -> F1: 0.8645, ROC-AUC: 0.8435\n",
      "\n",
      "=== Fold 7 ===\n",
      "Sex_F -> F1: 0.7035, ROC-AUC: 0.7385\n",
      "Outcome ADHD -> F1: 0.8683, ROC-AUC: 0.8212\n",
      "\n",
      "=== Fold 8 ===\n",
      "Sex_F -> F1: 0.6799, ROC-AUC: 0.7466\n",
      "Outcome ADHD -> F1: 0.8747, ROC-AUC: 0.8135\n",
      "\n",
      "=== Fold 9 ===\n",
      "Sex_F -> F1: 0.7082, ROC-AUC: 0.8193\n",
      "Outcome ADHD -> F1: 0.8855, ROC-AUC: 0.8496\n",
      "\n",
      "=== Fold 10 ===\n",
      "Sex_F -> F1: 0.6782, ROC-AUC: 0.7733\n",
      "Outcome ADHD -> F1: 0.9204, ROC-AUC: 0.8692\n",
      "\n",
      "=== Fold 11 ===\n",
      "Sex_F -> F1: 0.7075, ROC-AUC: 0.8055\n",
      "Outcome ADHD -> F1: 0.8811, ROC-AUC: 0.8338\n",
      "\n",
      "=== Fold 12 ===\n",
      "Sex_F -> F1: 0.6821, ROC-AUC: 0.7909\n",
      "Outcome ADHD -> F1: 0.8834, ROC-AUC: 0.8752\n",
      "\n",
      "=== Fold 13 ===\n",
      "Sex_F -> F1: 0.6783, ROC-AUC: 0.7587\n",
      "Outcome ADHD -> F1: 0.8828, ROC-AUC: 0.7824\n",
      "\n",
      "=== Fold 14 ===\n",
      "Sex_F -> F1: 0.6873, ROC-AUC: 0.7977\n",
      "Outcome ADHD -> F1: 0.8889, ROC-AUC: 0.8722\n",
      "\n",
      "=== Fold 15 ===\n",
      "Sex_F -> F1: 0.6534, ROC-AUC: 0.7184\n",
      "Outcome ADHD -> F1: 0.8809, ROC-AUC: 0.8141\n",
      "\n",
      "=== Fold 16 ===\n",
      "Sex_F -> F1: 0.6994, ROC-AUC: 0.7779\n",
      "Outcome ADHD -> F1: 0.8686, ROC-AUC: 0.8069\n",
      "\n",
      "=== Fold 17 ===\n",
      "Sex_F -> F1: 0.7227, ROC-AUC: 0.8106\n",
      "Outcome ADHD -> F1: 0.8642, ROC-AUC: 0.8140\n",
      "\n",
      "=== Fold 18 ===\n",
      "Sex_F -> F1: 0.7175, ROC-AUC: 0.7580\n",
      "Outcome ADHD -> F1: 0.8917, ROC-AUC: 0.8442\n",
      "\n",
      "=== Fold 19 ===\n",
      "Sex_F -> F1: 0.6777, ROC-AUC: 0.8020\n",
      "Outcome ADHD -> F1: 0.8917, ROC-AUC: 0.8422\n",
      "\n",
      "=== Fold 20 ===\n",
      "Sex_F -> F1: 0.6762, ROC-AUC: 0.7608\n",
      "Outcome ADHD -> F1: 0.8932, ROC-AUC: 0.8888\n",
      "\n",
      "=== Fold 21 ===\n",
      "Sex_F -> F1: 0.6762, ROC-AUC: 0.7749\n",
      "Outcome ADHD -> F1: 0.8730, ROC-AUC: 0.8326\n",
      "\n",
      "=== Fold 22 ===\n",
      "Sex_F -> F1: 0.7268, ROC-AUC: 0.7872\n",
      "Outcome ADHD -> F1: 0.8753, ROC-AUC: 0.8312\n",
      "\n",
      "=== Fold 23 ===\n",
      "Sex_F -> F1: 0.6927, ROC-AUC: 0.8005\n",
      "Outcome ADHD -> F1: 0.8846, ROC-AUC: 0.8277\n",
      "\n",
      "=== Fold 24 ===\n",
      "Sex_F -> F1: 0.7227, ROC-AUC: 0.7892\n",
      "Outcome ADHD -> F1: 0.8968, ROC-AUC: 0.8906\n",
      "\n",
      "=== Fold 25 ===\n",
      "Sex_F -> F1: 0.7024, ROC-AUC: 0.7911\n",
      "Outcome ADHD -> F1: 0.8773, ROC-AUC: 0.7960\n",
      "\n",
      "=== CV Results ===\n",
      "Sex Mean Brier Score: 0.6958\n",
      "Sex Mean F1: 0.7822\n",
      "ADHD Mean Brier Score: 0.8827\n",
      "ADHD Mean F1: 0.8370\n"
     ]
    }
   ],
   "source": [
    "#Actual Validation and Modeling\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Constants\n",
    "SEED = 42\n",
    "REPEATS = 5\n",
    "FOLDS = 5\n",
    "t_sex = 0.3\n",
    "t_adhd = 0.4\n",
    "\n",
    "# Evaluation function\n",
    "\n",
    "\n",
    "def eval_metrics(y_true, y_pred, weights, label=\"None\", thresh=0.5):\n",
    "    \"\"\"Evaluate predictions using F1 Score and ROC-AUC.\"\"\"\n",
    "    \n",
    "    # Calculate the ROC-AUC score (works with probabilities for multi-class classification too)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate the F1 score with thresholding, using weights for sample weighting\n",
    "    f1 = f1_score(y_true, (y_pred > thresh).astype(int), sample_weight=weights)\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"{label} -> F1: {f1:.4f}, ROC-AUC: {roc_auc:.4f}\")\n",
    "    \n",
    "    # Return the metrics\n",
    "    return f1, roc_auc\n",
    "\n",
    "\n",
    "# Initialize lists to store scores and out-of-fold predictions\n",
    "scores_sex = []\n",
    "scores_adhd = []\n",
    "sex_oof = np.zeros(len(y_Sex))\n",
    "adhd_oof = np.zeros(len(y_ADHD))\n",
    "\n",
    "# Cross-validation setup\n",
    "rskf = RepeatedStratifiedKFold(n_splits=FOLDS, n_repeats=REPEATS, random_state=SEED)\n",
    "skf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Logistic RegressionCV parameters\n",
    "logreg_params = {\n",
    "    \"penalty\": \"l2\",\n",
    "    \"Cs\": 10,\n",
    "    \"cv\": skf,\n",
    "    \"fit_intercept\": True,\n",
    "    \"scoring\": \"f1\",\n",
    "    \"random_state\": SEED,\n",
    "    \"solver\": \"saga\"\n",
    "    # \"max_iter\": 1000  # Increase this\n",
    "\n",
    "}\n",
    "\n",
    "model_sex = LogisticRegressionCV(**logreg_params)\n",
    "model_adhd = LogisticRegressionCV(**logreg_params)\n",
    "\n",
    "# Start cross-validation loop\n",
    "for fold, (train_idx, val_idx) in enumerate(rskf.split(train, combinations), 1):\n",
    "    print(f\"\\n=== Fold {fold} ===\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_val = train.iloc[train_idx].copy(), train.iloc[val_idx].copy()\n",
    "    y_train_sex, y_val_sex = y_Sex.iloc[train_idx], y_Sex.iloc[val_idx]\n",
    "    y_train_adhd, y_val_adhd = y_ADHD.iloc[train_idx], y_ADHD.iloc[val_idx]\n",
    "\n",
    "    # Sample weights\n",
    "    weights_train = np.where(combinations.iloc[train_idx] == \"11\", 2, 1)\n",
    "    weights_val = np.where(combinations.iloc[val_idx] == \"11\", 2, 1)\n",
    "\n",
    "    # Train model to predict Sex_F\n",
    "    model_sex.fit(X_train, y_train_sex, sample_weight=weights_train)\n",
    "    sex_train_pred = model_sex.predict_proba(X_train)[:, 1]\n",
    "    sex_val_pred = model_sex.predict_proba(X_val)[:, 1]\n",
    "    sex_oof[val_idx] += sex_val_pred / REPEATS\n",
    "\n",
    "    sex_f1, sex_roc_auc = eval_metrics(y_val_sex, sex_val_pred, weights_val, \"Sex_F\", thresh=t_sex)\n",
    "    scores_sex.append((sex_f1, sex_roc_auc))\n",
    "\n",
    "    # Add predicted sex probabilities\n",
    "    X_train = X_train.assign(sex_proba=sex_train_pred)\n",
    "    X_val = X_val.assign(sex_proba=sex_val_pred)\n",
    "\n",
    "    # Create interaction features\n",
    "    for col in interactions:\n",
    "        X_train[f\"I_{col}\"] = X_train[col] * X_train[\"sex_proba\"]\n",
    "        X_val[f\"I_{col}\"] = X_val[col] * X_val[\"sex_proba\"]\n",
    "\n",
    "    # Train model to predict ADHD outcome\n",
    "    model_adhd.fit(X_train[features_adhd], y_train_adhd, sample_weight=weights_train)\n",
    "    adhd_val_pred = model_adhd.predict_proba(X_val[features_adhd])[:, 1]\n",
    "    adhd_oof[val_idx] += adhd_val_pred / REPEATS\n",
    "\n",
    "    adhd_f1, adhd_roc_auc = eval_metrics(y_val_adhd, adhd_val_pred, weights_val, \"Outcome ADHD\", thresh=t_adhd)\n",
    "    scores_adhd.append((adhd_f1, adhd_roc_auc))\n",
    "\n",
    "# Print final results\n",
    "print(f\"\\n=== CV Results ===\")\n",
    "print(f\"Sex Mean roc auc Score: {np.mean([b for b, _ in scores_sex]):.4f}\")\n",
    "print(f\"Sex Mean F1: {np.mean([f for _, f in scores_sex]):.4f}\")\n",
    "print(f\"ADHD Mean roc auc Score: {np.mean([b for b, _ in scores_adhd]):.4f}\")\n",
    "print(f\"ADHD Mean F1: {np.mean([f for _, f in scores_adhd]):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Kaggle Description: \n",
    "Logistic Regression models used for predicting both 'Sex_F' and 'Outcome_ADHD' using Repeated Stratified K-Folds (5-fold, 5 repeats). Key tests included:\n",
    "1. Feature Engineering: Added interaction features using predicted sex probabilities, which improved ADHD prediction.\n",
    "2. Sex Prediction: The model showed fluctuations in performance, with F1 scores ranging from 0.6534 to 0.7268, but stable ROC-AUC.\n",
    "3. ADHD Prediction: Strong and consistent performance, with F1 scores mostly above 0.85 and high ROC-AUC.\n",
    "\n",
    "\n",
    "Logistic Regression models used for predicting both 'Sex_F' and 'Outcome_ADHD' using Repeated Stratified K-Folds (5-fold, 5 repeats). Key tests included:\n",
    "1. model outputs probabilities, initially in my last submission I made the threshold to consider sex to be female 0.4 (ADHD was 0.5), in this submission I made it 0.5 for sex\n",
    "1. Feature Engineering: Added interaction features using predicted sex probabilities, which improved ADHD prediction.\n",
    "2. Sex Prediction: The model showed fluctuations in performance, with F1 scores ranging from 0.6534 to 0.7268, but stable ROC-AUC.\n",
    "3. ADHD Prediction: Strong and consistent performance, with F1 scores mostly above 0.85 and high "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file sex7822adhd8370.csv created successfully!\n"
     ]
    }
   ],
   "source": [
    "    # ==========================================\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "\n",
    "    # Resulting file isn't in 1 or 0, it's in probabilities \n",
    "\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "\n",
    "\n",
    "\n",
    "# Logistic RegressionCV parameters\n",
    "logreg_params = {\n",
    "    \"penalty\": \"l2\",\n",
    "    \"Cs\": 10,\n",
    "    \"cv\": skf,\n",
    "    \"fit_intercept\": True,\n",
    "    \"scoring\": \"f1\",\n",
    "    \"random_state\": SEED,\n",
    "    \"solver\": \"saga\"\n",
    "    # \"max_iter\": 1000  # Increase this\n",
    "\n",
    "}\n",
    "\n",
    "model_sex = LogisticRegressionCV(**logreg_params)\n",
    "model_adhd = LogisticRegressionCV(**logreg_params)\n",
    "\n",
    "# Train the models on the full dataset (without cross-validation)\n",
    "model_sex.fit(train, y_Sex, sample_weight=np.where(combinations == \"11\", 2, 1))\n",
    "model_adhd.fit(train[features_adhd], y_ADHD, sample_weight=np.where(combinations == \"11\", 2, 1))\n",
    "\n",
    "# Predict on the test data\n",
    "sex_test_pred = model_sex.predict_proba(test)[:, 1]  # Sex_F predictions\n",
    "adhd_test_pred = model_adhd.predict_proba(test[features_adhd])[:, 1]  # ADHD predictions\n",
    "\n",
    "# Create submission DataFrame (adjust column names as needed)\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test.index,  \n",
    "    'adhd_prediction': adhd_test_pred,\n",
    "    'sex_prediction': sex_test_pred\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('sex7822adhd8370.csv', index=False)\n",
    "\n",
    "print(\"Submission file sex7822adhd8370.csv created successfully!\")\n",
    "\n",
    "# Kaggle Description: \n",
    "# Logistic Regression models used for predicting both 'Sex_F' and 'Outcome_ADHD' using Repeated Stratified K-Folds (5-fold, 5 repeats). Key tests included:\n",
    "# 1. Feature Engineering: Added interaction features using predicted sex probabilities, which improved ADHD prediction.\n",
    "# 2. Sex Prediction: The model showed fluctuations in performance, with F1 scores ranging from 0.6534 to 0.7268, but stable ROC-AUC.\n",
    "# 3. ADHD Prediction: Strong and consistent performance, with F1 scores mostly above 0.85 and high ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sex_test_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 13\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ==========================================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Convert probabilities to binary predictions (0 or 1)\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m sex_test_pred_binary \u001b[38;5;241m=\u001b[39m (\u001b[43msex_test_pred\u001b[49m \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.3\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     14\u001b[0m adhd_test_pred_binary \u001b[38;5;241m=\u001b[39m (adhd_test_pred \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Create submission DataFrame with hard labels\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sex_test_pred' is not defined"
     ]
    }
   ],
   "source": [
    "    # ==========================================\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "\n",
    "    # Convert results from probabilities to actual 1s or 0s\n",
    "\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "    # ==========================================\n",
    "\n",
    "\n",
    "\n",
    "# Convert probabilities to binary predictions (0 or 1)\n",
    "sex_test_pred_binary = (sex_test_pred >= 0.3).astype(int)\n",
    "adhd_test_pred_binary = (adhd_test_pred >= 0.4).astype(int)\n",
    "\n",
    "# Create submission DataFrame with hard labels\n",
    "submission = pd.DataFrame({\n",
    "    'participant_id': test.index,  \n",
    "    'adhd_prediction': adhd_test_pred_binary,\n",
    "    'sex_prediction': sex_test_pred_binary\n",
    "})\n",
    "\n",
    "# Save submission file\n",
    "submission.to_csv('Sex7822ADHD8370_ADHDThreshold4SexThreshold3.csv', index=False)\n",
    "print(\"Binary-label submission file created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary-label submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file with probabilities\n",
    "df = pd.read_csv('sex7822adhd8370.csv')\n",
    "\n",
    "# Apply thresholds to convert probabilities to binary labels\n",
    "df['adhd_prediction'] = (df['adhd_prediction'] >= 0.4).astype(int)\n",
    "df['sex_prediction'] = (df['sex_prediction'] >= 0.3).astype(int)\n",
    "\n",
    "# Save the new binary-label submission\n",
    "df.to_csv('Sex7822ADHD8370_ADHDThreshold4SexThreshold3.csv', index=False)\n",
    "\n",
    "print(\"Binary-label submission file created successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv2",
   "language": "python",
   "name": "venv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
